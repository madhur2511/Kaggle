{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Understand the problem. \n",
    "   We'll look at each variable and do a philosophical analysis about their meaning and importance for this problem.\n",
    "2. Univariable study. \n",
    "   We'll just focus on the dependent variable ('SalePrice') and try to know a little bit more about it.\n",
    "3. Multivariate study. \n",
    "   We'll try to understand how the dependent variable and independent variables relate.\n",
    "4. Basic cleaning. \n",
    "   We'll clean the dataset and handle the missing data, outliers and categorical variables.\n",
    "5. Test assumptions. \n",
    "   We'll check if our data meets the assumptions required by most multivariate techniques.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from sklearn import linear_model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    return pd.read_csv(file_path, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def univariate_analysis(df):\n",
    "    \"\"\"Univariate Analysis of target variable\"\"\"\n",
    "    \n",
    "    print df['SalePrice'].describe()\n",
    "    sns.distplot(df['SalePrice'])\n",
    "\n",
    "    # Positive skewed\n",
    "    print df['SalePrice'].skew()\n",
    "\n",
    "    # Positive kurtosis; more prone to outliers -> \"light-tailed\"\n",
    "    print df['SalePrice'].kurt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multivariate_analysis(df):\n",
    "    \"\"\"Multivariate Analysis\"\"\"\n",
    "    # Correlation matrix\n",
    "    corrmat = df.corr()\n",
    "    f, ax = plt.subplots(figsize=(12, 9))\n",
    "    sns.heatmap(corrmat, vmax=.8, square=True)\n",
    "    plt.show()\n",
    "\n",
    "    # SalePrice correlation matrix (zoomed)\n",
    "    k = 10 #number of variables for heatmap\n",
    "    cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "    cm = np.corrcoef(df[cols].values.T)\n",
    "    sns.set(font_scale=1.25)\n",
    "    hm = sns.heatmap(\n",
    "        cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, \n",
    "        yticklabels=cols.values, xticklabels=cols.values\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'\n",
    "    # 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def missing_data(df):\n",
    "    \"\"\"Missing data\"\"\"\n",
    "    # In this dataset, we dont really have any missing data, we conside NA as a separate category\n",
    "    perc_missing_values = train_X.isnull().sum() / train_X.shape[0]\n",
    "    perc_missing_values = perc_missing_values.sort_values(ascending=False)\n",
    "\n",
    "    # PoolQC, MiscFeature, Alley, Fence seem important variable and we wont remove them, \n",
    "    # instead, we shall convert them into boolean variables and use them in our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_massaging(df):    \n",
    "    # remove and replace some columns and values\n",
    "    df['LotFrontage'] = df['LotFrontage'].replace('NA', 0).astype('int64')\n",
    "    df['MasVnrArea'] = df['MasVnrArea'].replace('NA', 0).astype('int64')\n",
    "    df['TotalBsmtSF'] = df['TotalBsmtSF'].replace('NA', 0).astype('int64')\n",
    "    df['BsmtUnfSF'] = df['BsmtUnfSF'].replace('NA', 0).astype('int64')\n",
    "    df['GarageCars'] = df['GarageCars'].replace('NA', 0).astype('int64')\n",
    "    df['BsmtFullBath'] = df['BsmtFullBath'].replace('NA', 0).astype('int64')\n",
    "    df['BsmtHalfBath'] = df['BsmtHalfBath'].replace('NA', 0).astype('int64')\n",
    "\n",
    "    # new features\n",
    "    df['VintageBuilt'] = df['YearBuilt'].apply(lambda year: 1 if year < 1920 else 0)\n",
    "    df['LatestBuilt'] = df['YearBuilt'].apply(lambda year: 1 if year > 2000 else 0)\n",
    "    df['ageOfHouse'] = df['YrSold'] - df['YearBuilt']\n",
    "    df['ageOfHouseSinceRemodeling'] = df['YrSold'] - df['YearRemodAdd']\n",
    "    df['TotalFullBaths'] = df['BsmtFullBath'] + df['FullBath']\n",
    "    df['TotalHalfBaths'] = df['BsmtHalfBath'] + df['HalfBath']\n",
    "    df['TotalPorchArea'] = df['OpenPorchSF'] + df['EnclosedPorch'] + df['3SsnPorch'] + df['ScreenPorch']\n",
    "    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "\n",
    "#     df = df.drop('YearRemodAdd', 1)\n",
    "#     df = df.drop('YrSold', 1)\n",
    "#     df = df.drop('GarageYrBlt', 1)\n",
    "#     df = df.drop('MoSold', 1)\n",
    "\n",
    "    # Only keep total bsmt area\n",
    "#     df = df.drop('BsmtFinSF1', 1)\n",
    "#     df = df.drop('BsmtFinSF2', 1)\n",
    "#     df = df.drop('BsmtUnfSF', 1)\n",
    "    \n",
    "    # 1stFlrSF + 2ndFlrSF ~ GrLivArea\n",
    "#     df = df.drop('GrLivArea', 1)\n",
    "#     df = df.drop('OpenPorchSF', 1)\n",
    "#     df = df.drop('EnclosedPorch', 1)\n",
    "#     df = df.drop('3SsnPorch', 1)\n",
    "#     df = df.drop('ScreenPorch', 1)\n",
    "\n",
    "    # GarageCond and GarageQual are highly correlated\n",
    "#     df = df.drop('GarageCond', 1)\n",
    "#     df = df.drop('GarageArea', 1)\n",
    "\n",
    "    df = df.drop('Id', 1)\n",
    "\n",
    "    # transformed into categorical features.\n",
    "    df['MSSubClass'] = df['MSSubClass'].apply(str)\n",
    "    df['YrSold'] = df['YrSold'].astype(str)\n",
    "    df['MoSold'] = df['MoSold'].astype(str)\n",
    "    \n",
    "    # separate out numeric, categorical and ordinal data-frames\n",
    "    numeric_df = df.select_dtypes(include=['int64'])\n",
    "    categorical_df = df.select_dtypes(include=['object'])\n",
    "\n",
    "    ordinal_columns = [\n",
    "        'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n",
    "        'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'CentralAir', 'KitchenQual',\n",
    "        'FireplaceQu', 'GarageFinish', 'GarageQual', 'PavedDrive', 'PoolQC', 'Fence'\n",
    "    ]\n",
    "    ordinal_df = pd.DataFrame(categorical_df[ordinal_columns])\n",
    "\n",
    "    non_ordinal_columns = list(set(categorical_df.columns) - set(ordinal_columns))\n",
    "    non_ordinal_df = pd.DataFrame(categorical_df[non_ordinal_columns])\n",
    "    \n",
    "    # Replace ordinal categorical variables with numerical values\n",
    "    replace_rules = {\n",
    "        'ExterQual':    {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\":0},\n",
    "        'ExterCond':    {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1},\n",
    "        'BsmtQual':     {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        'BsmtCond':     {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        'BsmtExposure': {\"Gd\": 4, \"Av\": 3, \"Mn\": 2, \"No\": 1, \"NA\": 0},\n",
    "        'BsmtFinType1': {\"GLQ\": 6, \"ALQ\": 5, \"BLQ\": 4, \"Rec\": 3, \"LwQ\": 2, \"Unf\": 1, \"NA\": 0},\n",
    "        'BsmtFinType2': {\"GLQ\": 6, \"ALQ\": 5, \"BLQ\": 4, \"Rec\": 3, \"LwQ\": 2, \"Unf\": 1, \"NA\": 0},\n",
    "        'HeatingQC':    {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1},\n",
    "        'CentralAir':   {\"Y\": 1, \"N\": 0},\n",
    "        'KitchenQual':  {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        'FireplaceQu':  {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        'GarageFinish': {\"Fin\": 3, \"RFn\": 2, \"Unf\": 1, \"NA\": 0},\n",
    "        'GarageQual':   {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n",
    "        'PavedDrive':   {\"Y\": 2, \"P\": 1, \"N\": 0},\n",
    "        'PoolQC':       {\"Ex\": 4, \"Gd\": 3, \"TA\": 2, \"Fa\": 1, \"NA\": 0},\n",
    "        'Fence':        {\"GdPrv\": 4, \"MnPrv\": 3, \"GdWo\": 2, \"MnWw\": 1, \"NA\": 0},\n",
    "    }\n",
    "    ordinal_df.replace(replace_rules, inplace=True)\n",
    "    \n",
    "    # generate dummy variables for  non-ordinal categorical variables - ONE HOT ENCODING\n",
    "    dummy_non_ordinal_df = pd.get_dummies(non_ordinal_df)\n",
    "    \n",
    "    # numerical values\n",
    "    numeric_df = numeric_df\n",
    "    \n",
    "    # merge the dataframes\n",
    "    refined_df = pd.concat([ordinal_df, dummy_non_ordinal_df, numeric_df], axis=1)\n",
    "\n",
    "    assert df.shape[0] == refined_df.shape[0]\n",
    "    return refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_columns(df):\n",
    "    columns = sorted(df.columns)\n",
    "    return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensure_columns_are_same(train_X, test_X):\n",
    "    incorrect_train_columns, incorrect_test_columns = set(), set()\n",
    "    incorrect_train_columns |= (set(train_X.columns) - set(test_X.columns))\n",
    "    incorrect_test_columns |= (set(test_X.columns) - set(train_X.columns))\n",
    "    \n",
    "    train_X = train_X.drop(list(incorrect_train_columns), axis=1)\n",
    "    test_X = test_X.drop(list(incorrect_test_columns), axis=1)\n",
    "    \n",
    "    assert train_X.shape[1] == test_X.shape[1]\n",
    "    return sort_columns(train_X), sort_columns(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression(train_X, train_y, test_X):\n",
    "    train_X, test_X = ensure_columns_are_same(train_X, test_X)\n",
    "    lm = linear_model.LinearRegression()\n",
    "    lm.fit(train_X, np.log(train_y))\n",
    "    predicted_test_y = lm.predict(test_X)\n",
    "    return lm, np.exp(predicted_test_y), train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elasticnet_regression(alpha, l1_ratio, train_X, train_y, test_X):\n",
    "    train_X, test_X = ensure_columns_are_same(train_X, test_X)\n",
    "    lm = linear_model.ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "    lm.fit(train_X, np.log(train_y))\n",
    "    predicted_test_y = lm.predict(test_X)\n",
    "    return lm, np.exp(predicted_test_y), train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(alpha, train_X, train_y, test_X):\n",
    "    train_X, test_X = ensure_columns_are_same(train_X, test_X)\n",
    "    lm = linear_model.Ridge(alpha=alpha)\n",
    "    lm.fit(train_X, np.log(train_y))\n",
    "    predicted_test_y = lm.predict(test_X)\n",
    "    return lm, np.exp(predicted_test_y), train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lasso(alpha, train_X, train_y, test_X):\n",
    "    train_X, test_X = ensure_columns_are_same(train_X, test_X)\n",
    "    lm = linear_model.Lasso(alpha=alpha)\n",
    "    lm.fit(train_X, np.log(train_y))\n",
    "    predicted_test_y = lm.predict(test_X)\n",
    "    return lm, np.exp(predicted_test_y), train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest is a Bagging based model.\n",
    "\n",
    "def random_forest_regressor(train_X, train_y, test_X, **kwargs):\n",
    "    train_X, test_X = ensure_columns_are_same(train_X, test_X)\n",
    "    lm = RandomForestRegressor(\n",
    "        max_depth=kwargs['max_depth'],\n",
    "        max_features=kwargs['max_features'],\n",
    "        min_samples_split=kwargs['min_samples_split'],\n",
    "        n_estimators=kwargs['n_estimators'],\n",
    "        min_samples_leaf=kwargs['min_samples_leaf'],\n",
    "    )\n",
    "    lm.fit(train_X, np.log(train_y))\n",
    "    predicted_test_y = lm.predict(test_X)\n",
    "    return lm, np.exp(predicted_test_y), train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbm_regressor(train_X, train_y, test_X, **kwargs):\n",
    "    train_X, test_X = ensure_columns_are_same(train_X, test_X)\n",
    "    lm = GradientBoostingRegressor(\n",
    "        max_depth=kwargs['max_depth'],\n",
    "        max_features=kwargs['max_features'],\n",
    "        min_samples_split=kwargs['min_samples_split'],\n",
    "        n_estimators=kwargs['n_estimators'],\n",
    "        min_samples_leaf=kwargs['min_samples_leaf'],\n",
    "        loss=kwargs['loss'],\n",
    "        learning_rate=kwargs['learning_rate']\n",
    "    )\n",
    "    lm.fit(train_X, np.log(train_y))\n",
    "    predicted_test_y = lm.predict(test_X)\n",
    "    return lm, np.exp(predicted_test_y), train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_regressor(train_X, train_y, test_X, **kwargs):\n",
    "    train_X, test_X = ensure_columns_are_same(train_X, test_X)\n",
    "    try:\n",
    "        lm = XGBRegressor(\n",
    "            reg_alpha=kwargs['reg_alpha'],\n",
    "            colsample_bytree=kwargs['colsample_bytree'],\n",
    "            learning_rate=kwargs['learning_rate'],\n",
    "            min_child_weight=kwargs['min_child_weight'],\n",
    "            n_estimators=kwargs['n_estimators'],\n",
    "            subsample=kwargs['subsample'],\n",
    "            max_depth=kwargs['max_depth'],\n",
    "            gamma=kwargs['gamma']\n",
    "        )\n",
    "    except:\n",
    "        lm = XGBRegressor()\n",
    "    lm.fit(train_X, np.log(train_y))\n",
    "    predicted_test_y = lm.predict(test_X)\n",
    "    return lm, np.exp(predicted_test_y), train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSE(target, prediction):\n",
    "    return np.sqrt(((prediction - target) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_for_upload(predicted_test_y):\n",
    "    n = len(predicted_test_y)\n",
    "    predictions = pd.DataFrame()\n",
    "    predictions['Id'] = np.array([n + i + 1 for i in range(1, n + 1)])\n",
    "    predictions['SalePrice'] = predicted_test_y\n",
    "    predictions.to_csv('HousePrice_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file = 'train.csv'\n",
    "test_data_file = 'test.csv'\n",
    "\n",
    "train_X = read_data(training_data_file)\n",
    "test_X = read_data(test_data_file)\n",
    "\n",
    "refined_train_X = data_massaging(train_X)\n",
    "refined_test_X = data_massaging(test_X)\n",
    "\n",
    "refined_train_y = pd.DataFrame(refined_train_X['SalePrice'])\n",
    "refined_train_X = refined_train_X.drop('SalePrice', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>ExterCond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ExterQual</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExterCond</th>\n",
       "      <td>0.009184</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ExterQual  ExterCond\n",
       "ExterQual   1.000000   0.009184\n",
       "ExterCond   0.009184   1.000000"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115682590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "train_X['YearBuilt'].hist()\n",
    "\n",
    "refined_train_X[['ExterQual', 'ExterCond']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training data into train and validation sets into 2/3 and 1/3\n",
    "ref_train_X, ref_val_X, ref_train_y, ref_val_y = train_test_split(\n",
    "    refined_train_X,\n",
    "    refined_train_y,\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 28150.20\n",
      "Variance score: 0.89\n"
     ]
    }
   ],
   "source": [
    "model, predicted_val_y, _, _ = linear_regression(\n",
    "    ref_train_X,\n",
    "    ref_train_y,\n",
    "    ref_val_X,\n",
    ")\n",
    "print \"Mean squared error: %.2f\" % RMSE(ref_val_y, predicted_val_y)\n",
    "print \"Variance score: %.2f\" % r2_score(ref_val_y, predicted_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001 0.25\n",
      "Mean squared error: 27372.88\n",
      "Variance score: 0.90\n",
      "0.0001 0.5\n",
      "Mean squared error: 25691.87\n",
      "Variance score: 0.91\n",
      "0.0005 0.25\n",
      "Mean squared error: 28836.57\n",
      "Variance score: 0.89\n",
      "0.0005 0.5\n",
      "Mean squared error: 27302.48\n",
      "Variance score: 0.90\n",
      "0.001 0.25\n",
      "Mean squared error: 29431.04\n",
      "Variance score: 0.88\n",
      "0.001 0.5\n",
      "Mean squared error: 28559.08\n",
      "Variance score: 0.89\n",
      "0.005 0.25\n",
      "Mean squared error: 30279.78\n",
      "Variance score: 0.88\n",
      "0.005 0.5\n",
      "Mean squared error: 31118.63\n",
      "Variance score: 0.87\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.0001, 0.0005, 0.001, 0.005]:\n",
    "    for l1_ratio in [0.25, 0.5]:\n",
    "        model, predicted_val_y, _, _ = elasticnet_regression(\n",
    "            alpha,\n",
    "            l1_ratio,\n",
    "            ref_train_X,\n",
    "            ref_train_y,\n",
    "            ref_val_X,\n",
    "        )\n",
    "        print alpha, l1_ratio\n",
    "        print \"Mean squared error: %.2f\" % np.sqrt(mean_squared_error(ref_val_y, predicted_val_y))\n",
    "        print \"Variance score: %.2f\" % r2_score(ref_val_y, predicted_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "Mean squared error: 28159.23\n",
      "Variance score: 0.89\n",
      "0.005\n",
      "Mean squared error: 28197.41\n",
      "Variance score: 0.89\n",
      "0.01\n",
      "Mean squared error: 28248.99\n",
      "Variance score: 0.89\n",
      "0.05\n",
      "Mean squared error: 28721.86\n",
      "Variance score: 0.89\n",
      "0.1\n",
      "Mean squared error: 29282.29\n",
      "Variance score: 0.88\n",
      "0.5\n",
      "Mean squared error: 31204.51\n",
      "Variance score: 0.87\n",
      "1\n",
      "Mean squared error: 31649.75\n",
      "Variance score: 0.86\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]:\n",
    "    model, predicted_val_y, _, _ = ridge_regression(\n",
    "        alpha,\n",
    "        ref_train_X,\n",
    "        ref_train_y,\n",
    "        ref_val_X,\n",
    "    )\n",
    "    print alpha\n",
    "    print \"Mean squared error: %.2f\" % RMSE(ref_val_y, predicted_val_y)\n",
    "    print \"Variance score: %.2f\" % r2_score(ref_val_y, predicted_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "Mean squared error: 23656.73\n",
      "Variance score: 0.92\n",
      "0.0005\n",
      "Mean squared error: 24883.79\n",
      "Variance score: 0.92\n",
      "0.001\n",
      "Mean squared error: 28682.06\n",
      "Variance score: 0.89\n",
      "0.005\n",
      "Mean squared error: 31833.74\n",
      "Variance score: 0.86\n",
      "0.01\n",
      "Mean squared error: 33056.14\n",
      "Variance score: 0.85\n",
      "0.05\n",
      "Mean squared error: 35477.89\n",
      "Variance score: 0.83\n",
      "0.1\n",
      "Mean squared error: 37142.15\n",
      "Variance score: 0.81\n",
      "0.5\n",
      "Mean squared error: 35838.29\n",
      "Variance score: 0.83\n",
      "1\n",
      "Mean squared error: 35484.55\n",
      "Variance score: 0.83\n"
     ]
    }
   ],
   "source": [
    "for alpha in [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]:\n",
    "    model, predicted_val_y, _, _ = lasso(\n",
    "        alpha,\n",
    "        ref_train_X,\n",
    "        ref_train_y,\n",
    "        ref_val_X,\n",
    "    )\n",
    "    print alpha\n",
    "    print \"Mean squared error: %.2f\" % np.sqrt(mean_squared_error(ref_val_y, predicted_val_y))\n",
    "    print \"Variance score: %.2f\" % r2_score(ref_val_y, predicted_val_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 34225.98\n",
      "Variance score: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "random_forest_parameters = {\n",
    "    'max_features': 0.5,\n",
    "    'min_samples_split': 2,\n",
    "    'n_estimators': 20,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_leaf': 1\n",
    "}\n",
    "model, predicted_val_y, _, _ = random_forest_regressor(\n",
    "    ref_train_X,\n",
    "    ref_train_y,\n",
    "    ref_val_X,\n",
    "    **random_forest_parameters\n",
    ")\n",
    "print \"Mean squared error: %.2f\" % np.sqrt(mean_squared_error(ref_val_y, predicted_val_y))\n",
    "print \"Variance score: %.2f\" % r2_score(ref_val_y, predicted_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 29987.02\n",
      "Variance score: 0.88\n"
     ]
    }
   ],
   "source": [
    "gbm_parameters = {\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_split': 2,\n",
    "    'n_estimators': 4000,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'loss': 'ls',\n",
    "    'learning_rate': 0.01\n",
    "}\n",
    "model, predicted_val_y, _, _ = gbm_regressor(\n",
    "    ref_train_X,\n",
    "    ref_train_y,\n",
    "    ref_val_X,\n",
    "    **gbm_parameters\n",
    ")\n",
    "print \"Mean squared error: %.2f\" % np.sqrt(mean_squared_error(ref_val_y, predicted_val_y))\n",
    "print \"Variance score: %.2f\" % r2_score(ref_val_y, predicted_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 31614.79\n",
      "Variance score: 0.86\n"
     ]
    }
   ],
   "source": [
    "xgb_parameters = dict(\n",
    "     colsample_bytree=0.2,\n",
    "     gamma=0.0,\n",
    "     learning_rate=0.01,\n",
    "     max_depth=4,\n",
    "     min_child_weight=1.5,\n",
    "     n_estimators=7200,                                                                  \n",
    "     reg_alpha=0.9,\n",
    "     reg_lambda=0.6,\n",
    "     subsample=0.2,\n",
    "     seed=42,\n",
    "     silent=1\n",
    ")\n",
    "\n",
    "model, predicted_val_y, _, _ = xgb_regressor(\n",
    "    ref_train_X,\n",
    "    ref_train_y,\n",
    "    ref_val_X,\n",
    "    **xgb_parameters\n",
    ")\n",
    "print \"Mean squared error: %.2f\" % np.sqrt(mean_squared_error(ref_val_y, predicted_val_y))\n",
    "print \"Variance score: %.2f\" % r2_score(ref_val_y, predicted_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_to_left = st.beta(10, 1)  \n",
    "from_zero_positive = st.expon(0, 50)\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": st.randint(3, 40),\n",
    "    \"max_depth\": st.randint(3, 40),\n",
    "    \"learning_rate\": st.uniform(0.05, 0.4),\n",
    "    \"colsample_bytree\": one_to_left,\n",
    "    \"subsample\": one_to_left,\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "    'reg_alpha': from_zero_positive,\n",
    "    \"min_child_weight\": from_zero_positive,\n",
    "}\n",
    "\n",
    "xgbreg = XGBRegressor(nthreads=-1)\n",
    "\n",
    "gs = RandomizedSearchCV(xgbreg, xgb_param_grid, n_jobs=1)  \n",
    "gs.fit(refined_train_X, refined_train_y)  \n",
    "grid_search_report(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest grid search; use a full grid over all parameters\n",
    "rf_param_grid = {\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"max_features\": [1, 3, 10],\n",
    "    \"min_samples_split\": [2, 3, 10],\n",
    "    \"min_samples_leaf\": [1, 3, 10],\n",
    "    \"n_estimators\": [20, 50, 70,],\n",
    "}\n",
    "\n",
    "lm = RandomForestRegressor()\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(lm, param_grid=rf_param_grid)\n",
    "grid_search.fit(refined_train_X, refined_train_y)\n",
    "grid_search_report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, predicted_test_y, refined_train_X, refined_test_X = linear_regression(\n",
    "    refined_train_X,\n",
    "    refined_train_y,\n",
    "    refined_test_X,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0001\n",
    "l1_ratio = 0.5\n",
    "model, predicted_test_y, refined_train_X, refined_test_X = elasticnet_regression(\n",
    "    alpha,\n",
    "    l1_ratio,\n",
    "    refined_train_X,\n",
    "    refined_train_y,\n",
    "    refined_test_X,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.005\n",
    "model, predicted_test_y, refined_train_X, refined_test_X = ridge_regression(\n",
    "    alpha,\n",
    "    refined_train_X,\n",
    "    refined_train_y,\n",
    "    refined_test_X,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.0001\n",
    "model, predicted_test_y, refined_train_X, refined_test_X = lasso(\n",
    "    alpha,\n",
    "    refined_train_X,\n",
    "    refined_train_y,\n",
    "    refined_test_X,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "random_forest_parameters = {\n",
    "    'max_features': 0.5,\n",
    "    'min_samples_split': 2,\n",
    "    'n_estimators': 20,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_leaf': 1\n",
    "}\n",
    "model, predicted_test_y, refined_train_X, refined_test_X = random_forest_regressor(\n",
    "    refined_train_X,\n",
    "    refined_train_y,\n",
    "    refined_test_X,\n",
    "    **random_forest_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_parameters = {\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_split': 2,\n",
    "    'n_estimators': 4000,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'loss': 'ls',\n",
    "    'learning_rate': 0.01\n",
    "}\n",
    "model, predicted_test_y, refined_train_X, refined_test_X = gbm_regressor(\n",
    "    refined_train_X,\n",
    "    refined_train_y,\n",
    "    refined_test_X,\n",
    "    **gbm_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_parameters = dict(\n",
    "     colsample_bytree=0.2,\n",
    "     gamma=0.0,\n",
    "     learning_rate=0.01,\n",
    "     max_depth=4,\n",
    "     min_child_weight=1.5,\n",
    "     n_estimators=7200,                                                                  \n",
    "     reg_alpha=0.9,\n",
    "     reg_lambda=0.6,\n",
    "     subsample=0.2,\n",
    "     seed=42,\n",
    "     silent=1\n",
    ")\n",
    "model, predicted_test_y, refined_train_X, refined_test_X = xgb_regressor(\n",
    "    refined_train_X,\n",
    "    refined_train_y,\n",
    "    refined_test_X,\n",
    "    **xgb_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final model to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_for_upload(predicted_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459,)"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try correlation analysis between some seemingly similar features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
