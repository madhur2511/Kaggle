{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "** Case Deletion (CD) ** - Also is known as complete case analysis. It is available in all statistical packages and is the default method in many programs. This method consists of discarding all instances (cases) with missing values for at least one feature.A variation of this method consists of determining the extent of missing data on each instance and attribute and delete the instances and/or attributes with high\n",
    "levels of missing data. Before deleting any attribute, it is necessary to evaluate its relevance to the analysis.\n",
    "\n",
    "** Mean Imputation (MI) ** - This is one of the most frequently used methods.It consists of replacing the missing data for a given feature (attribute) by the mean of all known values of that attribute in the class where the instance with missing attribute belongs.\n",
    "\n",
    "** Median Imputation (MDI) ** - Since the mean is affected by the presence of outliers it seems natural to use the median instead just to assure robustness. In this case, the missing data for a given feature is replaced by the median of all known values of that attribute in the class where the instance with the missing feature belongs. This method is also a recommended choice when the distribution of the values of a given feature is skewed.\n",
    "\n",
    "** KNN Imputation (KNNI) ** - This method the missing values of an instance are imputed considering a given number of instances that are most similar to the instance of interest. The similarity of two instances is determined using a distance function.\n",
    "\n",
    "Also, \n",
    "1. https://pypi.python.org/pypi/fancyimpute\n",
    "2. https://machinelearningmastery.com/handle-missing-data-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent v/s Gradient Descent\n",
    "\n",
    "\n",
    "Batch Gradient Descent has to scan through the entire training set before taking a single \n",
    "step — a costly operation if m is large — Stochastic Gradient Descent can start making progress \n",
    "right away, and continues to make progress with each example it looks at. \n",
    "\n",
    "Often, stochastic gradient descent gets θ “close” to the minimum much faster than batch \n",
    "gradient descent. (Note however that it may never “converge” to the minimum, and the parameters\n",
    "θ will keep oscillating around the minimum of J(θ); but in practice most of the values near the \n",
    "minimum will be reasonably good approximations to the true minimum.2) \n",
    "\n",
    "For these reasons, particularly when the training set is large, stochastic gradient descent \n",
    "is often preferred over batch gradient descent.\n",
    "\n",
    "\n",
    "Note - \n",
    "While it is more common to run stochastic gradient descent as we have described it and with a \n",
    "fixed learning rate α, by slowly letting the learning rate α decrease to zero as the algorithm \n",
    "runs, it is also possible to ensure that the parameters will converge to the global minimum rather\n",
    "then merely oscillate around the minimum.\n",
    "\n",
    "Creds: Andrew Ng\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Bias-Variance Tradeoff\n",
    "\n",
    "The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an\n",
    "algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "= Mean(y - y^)\n",
    "\n",
    "The variance is error from sensitivity to small fluctuations in the training set. High variance \n",
    "can cause overfitting: modeling the random noise in the training data, rather than the intended \n",
    "outputs.\n",
    "\n",
    "**  The more complex the model f(x) is, the more data points it will capture, and the lower the bias\n",
    "will be. However, complexity will make the model \"move\" more to capture the data points, and hence\n",
    "its variance will be larger.  **\n",
    "\n",
    "Creds: Wikipedia\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "## Bias-Variance Tradeoff DT, RF, GBM\n",
    "\n",
    "Shallow decision trees have high bias and low variance.\n",
    "Deep decision trees have low bias and high variance. \n",
    "\n",
    "\n",
    "\n",
    "** Bagging **\n",
    "\n",
    "Before we talk about RFs, let’s first talk about Bagging. \n",
    "Bagging is a simpler version of RFs and can be described as follows:\n",
    "\n",
    "1. Create a new training set by sampling the original training set with replacement. \n",
    "The new training set has the same number of data points as the original, so there will \n",
    "generally be duplicates. This process is known as bootstrapping. \n",
    "\n",
    "2. Bootstrap a bunch of times, say 500. Train a DT on each bootstrap.\n",
    "\n",
    "3. Use all 500 DTs to make predictions. For regression, this can be simply the average. \n",
    "For classification, majority voting.\n",
    "\n",
    "Bagging stands for Bootstrap Aggregation, and can be applied to any model class, although \n",
    "historically it was most often used with DTs. \n",
    "\n",
    "** The key intuition of Bagging is that it reduces the variance of your model class. ** \n",
    "\n",
    "If you think about the simple 1-D regression example from above, \n",
    "Bagging is trying to get the prediction as close to the black line as possible. So when you use Bagging, \n",
    "you’re incentivized to use deep decision trees because they have high variance and low bias.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "** Random Forests **\n",
    "\n",
    "A Random Forest is a generalization of Bagging that is specific to DTs. At each branch in the \n",
    "decision tree, Random Forest training also subsamples the features in addition to the training examples.\n",
    "Intuitively, this process further de-correlates the individual trees, which is good for Bagging, since \n",
    "the main limitation of Bagging is that bootstrapping is not the same as drawing fresh samples from the \n",
    "true data distribution.\n",
    "\n",
    "\n",
    "\n",
    "** Boosting **\n",
    "\n",
    "There are a few different boosting methods around, including AdaBoost and Gradient Boosting. All of them take\n",
    "the same high-level approach:\n",
    "\n",
    "1. Keep an overall predictor that is the (weighted) average of a bunch of models.\n",
    "\n",
    "2. Train first model on original training data, and initialize overall predictor as just this single model.\n",
    "\n",
    "3. Assess the error of the the overall predictor and modify the training data the focus on areas of high error.\n",
    "\n",
    "   a) For AdaBoost, this means re-weighting the data points so that poorly modeled data points get higher weight.\n",
    "   \n",
    "   b) For Gradient Boosting, this means redefining the supervised prediction target to be some kind of residual \n",
    "   between the ground truth and the overall predictor.\n",
    "   \n",
    "4. Train a new model on the modified training data, and add to the overall predictor.\n",
    "Repeat Steps 3 & 4.\n",
    "\n",
    "One can interpret boosting as trying to minimize the bias of the overall predictor. So when you use boosting, \n",
    "you’re incentivized to use shallow decision trees because they have low variance and high bias. Using high variance\n",
    "base models in boosting runs a much higher risk of overfitting than approaches like Bagging.\n",
    "\n",
    "\n",
    "\n",
    "** Summary **\n",
    "\n",
    "Bagging and RFs try to create multiple representative data sets, and then predict the average of the models \n",
    "trained on these data sets. This process is exactly what you want to do if you want to minimize the variance \n",
    "& overfitting of your model class, but does nothing about minimizing bias. On the other hand, Boosting surgically\n",
    "manipulates the training set to focus on areas of high error. This is exactly what you’d do if you worry about \n",
    "your model class having high bias and being unable to globally model the data distribution well. However, Boosting\n",
    "itself ignores the overfitting issue. So when you use Bagging & RFs, try to use high variance & low bias models.\n",
    "Conversely, when you use Boosting, try to use low variance & high bias models.\n",
    "\n",
    "Creds: https://www.quora.com/What-are-the-differences-between-Random-Forest-and-Gradient-Tree-Boosting-algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "\n",
    "** GBM : ** \n",
    "1. https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "\n",
    "** XGBoost **\n",
    "1. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "2. https://www.kaggle.com/babatee/intro-xgboost-classification\n",
    "\n",
    "\n",
    "** LightGBM : ** \n",
    "1. https://github.com/Microsoft/LightGBM/blob/master/docs/Features.rst#leaf-wise-best-first-tree-growth\n",
    "2. https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-Tuning.rst\n",
    "3. https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/\n",
    "4. https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide\n",
    "\n",
    "\n",
    "\n",
    "** Why LightGBM is faster than GBM (XGBoost) and when to use which **\n",
    "\n",
    "XGBoost is pretty much a faster and regularized version of GBM and both of these use a *level-order* \n",
    "approach to grow the trees. In this strategy, each node splits the data prioritizing the nodes \n",
    "closer to the tree root.\n",
    "\n",
    "However, LightGBM uses a *leaf-level* tree growth strategy, where the tree grows by splitting the data at the nodes with the highest loss change. This could result in imbalance and deep trees, resulting in overfitting, hence controlling the *max_depth* parameter is important.\n",
    "\n",
    "Level-wise growth is usually better for smaller datasets whereas leaf-wise tends to overfit. Leaf-wise growth tends to excel in larger datasets where it is considerably faster than level-wise growth.\n",
    "\n",
    "Creds: https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Linear Models **\n",
    "1. http://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n",
    "** Feature Engineering **\n",
    "1. https://elitedatascience.com/feature-engineering-best-practices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
